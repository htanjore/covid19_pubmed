{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "import requests\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_rows = 1500\n",
    "pd.options.display.max_columns = 1500\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(search_for, filename):\n",
    "    #search_for = input(\"Enter search:\")\n",
    "\n",
    "    #filename = input(\"Enter file name:\")\n",
    "    \"\"\" 1. This function takes two arguments a) search item, b)filename to be saved\n",
    "        2. Searches the pubmed that gives keys a) query key and b) webenv key\n",
    "        3. Creates a new output folder and fetches files from the server using the keys and saves xml as .txt.\n",
    "            and prints the total number of records retreived\"\"\"\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&usehistory=y&retmax=99999&term=\"+search_for\n",
    "    response = requests.get(url)\n",
    "    search = BeautifulSoup(response.content, 'xml')\n",
    "    total_ids_search = int(search.find('Count').text)\n",
    "    webenv = search.find('WebEnv').text\n",
    "    query_key = search.find('QueryKey').text\n",
    "    get_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&query_key=1&webenv=\"+webenv\n",
    "    item = '0'\n",
    "    all_text = []\n",
    "    for item in tqdm(range(0, total_ids_search, 10000)):\n",
    "        get = get_url+\"&retstart=\"+str(item)\n",
    "        #get = get_url+\"&retmax=\"+number+\"&retstart=\"+str(item)\n",
    "        get_response = requests.post(get).text\n",
    "        all_text.append(get_response)\n",
    "        for index, text in enumerate(all_text):\n",
    "            if not os.path.isdir('../data/output'):\n",
    "                os.mkdir('../data/output')\n",
    "            with open(\"../data/output/\"+filename+str(index)+\".txt\", \"w\") as text_file:\n",
    "                time.sleep(0.1)\n",
    "                text_file.write(text)\n",
    "                text_file.close()\n",
    "\n",
    "        print(\"Total Number of records found :\"+str(total_ids_search))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataframe(filename, tocsv):\n",
    "    #name = input(\"Enter file with txt extension:\")\n",
    "\n",
    "    #tocsv = input(\"name to save:\")\n",
    "    \"\"\" 1.This function reads filename, that the user provided with an extension .txt generated from pubmed after get_data(search_for, filename) function.\n",
    "        2.Then parses the xml file and creates a lists of PMID, Article_title, ISOAbbreviation, Journal_title,\n",
    "         Abstract, Journal_Country,Published_year, Keyword_list,publication_type,Medlinecitation,pubmed_year,Affiliation\n",
    "        3.Take the above list and creates a dataframe and returns a dataframe\n",
    "        4.Finally the datafrane is saved into a csv file\"\"\"\n",
    "\n",
    "    files = glob.glob('../data/output/*.txt')\n",
    "    PMID = []\n",
    "    year=[]\n",
    "    ISO = []\n",
    "    Article_title = []\n",
    "    Journal_Country=[]\n",
    "    Journal_title=[]\n",
    "    abstract = []\n",
    "    keywords=[]\n",
    "    Medlinecitation = []\n",
    "    pubmed_year =[]\n",
    "    pubtype=[]\n",
    "    affiliation=[]\n",
    "    for file in tqdm(files):\n",
    "        with open(file, 'r') as reader:\n",
    "            contents = reader.read()\n",
    "            soup = BeautifulSoup(contents, 'xml')\n",
    "            root = soup.find_all('PubmedArticle')\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "            notuseful_list = ['Research Support', \"U.S. Gov't\",\"Non-U.S. Gov't\",\"Research Support, Non-U.S. Gov't\",\n",
    "           \"Research Support, N.I.H., Extramural\", \"Research Support, U.S. Gov't, Non-P.H.S.\",\n",
    "           \"Research Support, N.I.H., Extramural,Research Support, U.S. Gov't, Non-P.H.S.\" ,\n",
    "           \"Research Support, N.I.H., Intramural\", \"Research Support, U.S. Gov't, P.H.S.\" ] \n",
    "\n",
    "\n",
    "            for item in root:\n",
    "                pmid =  item.find('PMID')\n",
    "                pmid_text= pmid.text\n",
    "                PMID.append(pmid_text)\n",
    "                title = item.find('ArticleTitle')\n",
    "                title_text = title.text\n",
    "                Article_title.append(title_text)\n",
    "            for item in root:\n",
    "                iso_abbreviation = item.find('ISOAbbreviation')\n",
    "                if iso_abbreviation is not None:\n",
    "                    iso_abbreviation_text = iso_abbreviation.text\n",
    "                    ISO.append(iso_abbreviation_text)\n",
    "\n",
    "            for item in root:\n",
    "                if item is not None:\n",
    "                    journal = item.find('Journal')\n",
    "                    journal_name = journal.find_all('Title')\n",
    "                    for item in journal_name:\n",
    "                        journal_name_list = item.string\n",
    "                        Journal_title.append(journal_name_list)\n",
    "                else:\n",
    "                     Journal_title.append(None)  \n",
    "\n",
    "            all_Year_info =[]\n",
    "            for item in root:\n",
    "                all_Year_info =[]\n",
    "                year_pub =  item.find_all('PubDate')\n",
    "                year_pub_text = year_pub[0].text\n",
    "                all_Year_info.append(year_pub_text)\n",
    "                s = ''.join(all_Year_info)\n",
    "            for item in re.findall('(\\d{4})', s):\n",
    "                year.append(item.strip())\n",
    "\n",
    "            for item in root:\n",
    "                year_pub =  item.find(PubStatus=\"pubmed\")\n",
    "                if year_pub is not None:\n",
    "                    year1 =  year_pub.find_all('Year')\n",
    "                    for i in year1:\n",
    "                        pubmed_year.append(i.text)\n",
    "\n",
    "            \n",
    "            for item in root:\n",
    "                pub = item.find('PublicationTypeList')\n",
    "                if pub is not None:\n",
    "                    pub_lst=[]\n",
    "                    pubtype_list = pub.find_all('PublicationType')\n",
    "                    for item in pubtype_list:\n",
    "                        pubtype_text = item.text\n",
    "                        pub_lst.append(pubtype_text)\n",
    "                    pub_lst = [x for x in pub_lst if x.strip() not in notuseful_list]\n",
    "                    pubs_join= ','.join(pub_lst)\n",
    "                    pubtype.append(pubs_join)\n",
    "                else:\n",
    "                    pubtype.append(None)      \n",
    "\n",
    "            for item in root:\n",
    "                journal_country = item.find('MedlineJournalInfo')\n",
    "                if journal_country is not None:\n",
    "                    country_list = journal_country.find_all('Country')\n",
    "                    for item in country_list:\n",
    "                        country_list=item.text\n",
    "                        Journal_Country.append(country_list)\n",
    "                else:\n",
    "                    Journal_Country.append(None)\n",
    "\n",
    "            for item in root:\n",
    "                abstract_text = item.find('Abstract')\n",
    "                if abstract_text is not None:\n",
    "                    text = abstract_text.find_all('AbstractText')\n",
    "                    lst = []\n",
    "                    for item in text:\n",
    "                        lst.append(item.text)\n",
    "                    lst_join='\\n'.join(lst)\n",
    "                    abstract.append(lst_join)\n",
    "                else:\n",
    "                     abstract.append(None) \n",
    "\n",
    "            for item in root:\n",
    "                keyword_text=item.find('KeywordList')\n",
    "                if keyword_text is not None:\n",
    "                    key=[]\n",
    "                    keyword_text_list=keyword_text.find_all('Keyword')\n",
    "                    for item in keyword_text_list:\n",
    "                        keyword_text=item.text\n",
    "                        key.append(keyword_text)\n",
    "                    keys_join=','.join(key)\n",
    "                    keywords.append(keys_join)\n",
    "                else:\n",
    "                    keywords.append(None)\n",
    "\n",
    "            for item in soup.find_all('MedlineCitation'):\n",
    "                status = item.get('Status')\n",
    "                Medlinecitation.append(status)    \n",
    "\n",
    "            \n",
    "            for item in root:\n",
    "                abstract_text = item.find('AuthorList')\n",
    "                if abstract_text is not None:\n",
    "                    text = abstract_text.find_all('Affiliation')\n",
    "                    lst = []\n",
    "                    for item in text:\n",
    "                        lst.append(item.text)\n",
    "                    lst_join='\\n'.join(lst).replace(\"\\n\",\"\")\n",
    "                    affiliation.append(lst_join)\n",
    "                else:\n",
    "                    affiliation.append(None)\n",
    "\n",
    "            dict_columns = {'PMID': PMID,\n",
    "               'Title': Article_title,\n",
    "                'ISOAbbreviation': ISO,\n",
    "               'journal_title':Journal_title,\n",
    "                 'Abstract':abstract,\n",
    "                 'Journalinfo_country': Journal_Country,\n",
    "                  'Published_year':year,\n",
    "                   'Keyword_list':keywords,\n",
    "                  'publication_type':pubtype,\n",
    "                  'medline_citation':Medlinecitation,\n",
    "                  \"pubmed_year\":pubmed_year,\n",
    "                  \"Affiliation\":affiliation}\n",
    "\n",
    "            df =pd.DataFrame.from_dict(dict_columns, orient='index').transpose()\n",
    "            df.to_csv('../data/output/'+tocsv+'.csv',index=False)\n",
    "            print(\"Number of articles :\"+str(len(root)))\n",
    "            #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search For:-novel coronavirus or covid-19 or coronavirus disease-2019\n",
      "File name to xml output:-covid19_\n",
      "file name to save as csv:covid_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:57<02:53, 57.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :36553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [01:55<01:55, 57.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :36553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [02:57<00:58, 58.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :36553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:45<00:00, 56.31s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :36553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 1/4 [01:30<04:30, 90.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [03:00<03:00, 90.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [04:17<01:26, 86.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :6549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [06:02<00:00, 90.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_item = input(\"Search For:-\")\n",
    "filename =input(\"File name to xml output:-\")\n",
    "tocsv = input(\"file name to save as csv:\")\n",
    "#get_data(search_for, filename) # search anything in pubmed AND prints number of records\n",
    "get_data(search_item,filename)\n",
    "get_dataframe(filename, tocsv) #give file name from get_data with txt extension like pubmed_result.txt and assign the function to a variable\n",
    "#covid=get_dataframe('covid_19.txt', 'covid_19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('deeplearning': conda)",
   "language": "python",
   "name": "python37564bitdeeplearningconda81070860cf3b4c5d87322ab1a4573520"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search For:-novel coronavirus or covid-19 or coronavirus disease-2019\n",
      "File name to xml output:-covid19_42022_\n",
      "file name to save as csv:covid19_42022_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▊                                                                       | 1/26 [00:29<12:18, 29.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|█████▋                                                                    | 2/26 [01:07<13:48, 34.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|████████▌                                                                 | 3/26 [01:40<12:58, 33.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|███████████▍                                                              | 4/26 [02:20<13:15, 36.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████▏                                                           | 5/26 [02:56<12:42, 36.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████████████                                                         | 6/26 [03:32<12:02, 36.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████████████▉                                                      | 7/26 [04:08<11:23, 35.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|██████████████████████▊                                                   | 8/26 [04:46<11:03, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████▌                                                | 9/26 [05:27<10:45, 37.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████                                             | 10/26 [06:08<10:21, 38.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████████████████▉                                          | 11/26 [06:47<09:43, 38.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████▋                                       | 12/26 [07:21<08:46, 37.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████▌                                    | 13/26 [07:58<08:04, 37.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|███████████████████████████████████████▎                                 | 14/26 [08:41<07:47, 38.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|██████████████████████████████████████████                               | 15/26 [09:19<07:04, 38.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████▉                            | 16/26 [09:58<06:27, 38.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████████████▋                         | 17/26 [10:33<05:38, 37.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████████████████████████████████████████████████▌                      | 18/26 [11:11<05:03, 37.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|█████████████████████████████████████████████████████▎                   | 19/26 [11:51<04:29, 38.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████████████████████████▏                | 20/26 [12:28<03:47, 37.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|██████████████████████████████████████████████████████████▉              | 21/26 [13:02<03:04, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████▊           | 22/26 [13:38<02:26, 36.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████▌        | 23/26 [14:13<01:48, 36.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████▍     | 24/26 [14:49<01:12, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████▏  | 25/26 [16:19<00:52, 52.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 26/26 [16:32<00:00, 38.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records found :252002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|██▋                                                                       | 1/27 [00:31<13:28, 31.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▍                                                                    | 2/27 [01:01<12:43, 30.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▏                                                                 | 3/27 [01:27<11:31, 28.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████████▉                                                               | 4/27 [02:00<11:36, 30.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█████████████▋                                                            | 5/27 [02:32<11:20, 30.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|████████████████▍                                                         | 6/27 [03:07<11:19, 32.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████████████████▏                                                      | 7/27 [03:37<10:30, 31.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████████████                                                  | 8/27 [19:12<1:41:03, 319.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████▋                                               | 9/27 [19:38<1:08:16, 227.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████▋                                             | 10/27 [20:46<50:31, 178.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████████████████▎                                          | 11/27 [21:14<35:15, 132.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████                                       | 12/27 [35:12<1:26:41, 346.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|██████████████████████████████████▋                                     | 13/27 [35:23<57:11, 245.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████████████████████████████████████▎                                  | 14/27 [35:53<39:03, 180.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████████████████                                | 15/27 [36:24<27:04, 135.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|██████████████████████████████████████████▋                             | 16/27 [38:09<23:07, 126.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████████████▉                           | 17/27 [38:15<15:00, 90.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████                        | 18/27 [41:21<17:49, 118.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████████████████████████████████████████████████▎                     | 19/27 [41:56<12:28, 93.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████▎                  | 20/27 [44:06<12:11, 104.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████▍               | 21/27 [1:05:30<45:51, 458.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████             | 22/27 [1:06:07<27:40, 332.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|███████████████████████████████████████████████████████████▋          | 23/27 [1:06:46<16:15, 243.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|██████████████████████████████████████████████████████████████▏       | 24/27 [1:15:41<16:34, 331.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████████████████████████████▊     | 25/27 [1:20:33<10:39, 319.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████▍  | 26/27 [1:21:14<03:56, 236.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 27/27 [1:21:56<00:00, 182.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles :9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "import requests\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_rows = 1500\n",
    "pd.options.display.max_columns = 1500\n",
    "\n",
    "\n",
    "search_item = input(\"Search For:-\")\n",
    "filename =input(\"File name to xml output:-\")\n",
    "tocsv = input(\"file name to save as csv:\")\n",
    "\n",
    "\n",
    "def get_data(search_for, filename):\n",
    "    \"\"\" 1. This function takes two arguments a) search item, b)filename to be saved\n",
    "        2. Searches the pubmed that gives keys a) query key and b) webenv key\n",
    "        3. Creates a new output folder and fetches files from the server using the keys and saves xml as .txt.\n",
    "            and prints the total number of records retreived\"\"\"\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&usehistory=y&retmax=99999&term=\"+search_for\n",
    "    response = requests.get(url)\n",
    "    search = BeautifulSoup(response.content, 'xml')\n",
    "    total_ids_search = int(search.find('Count').text)\n",
    "    webenv = search.find('WebEnv').text\n",
    "    query_key = search.find('QueryKey').text\n",
    "    get_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&query_key=1&webenv=\"+webenv\n",
    "    item = '0'\n",
    "    all_text = []\n",
    "    for item in tqdm(range(0, total_ids_search, 10000)):\n",
    "        get = get_url+\"&retstart=\"+str(item)\n",
    "        #get = get_url+\"&retmax=\"+number+\"&retstart=\"+str(item)\n",
    "        get_response = requests.post(get).text\n",
    "        all_text.append(get_response)\n",
    "        for index, text in enumerate(all_text):\n",
    "            if not os.path.isdir('../data/output'):\n",
    "                os.mkdir('../data/output')\n",
    "            with open(\"../data/output/\"+filename+str(index)+\".txt\", \"w\") as text_file:\n",
    "                time.sleep(0.1)\n",
    "                text_file.write(text)\n",
    "                text_file.close()\n",
    "\n",
    "        print(\"Total Number of records found :\"+str(total_ids_search))\n",
    "        \n",
    "get_data(search_item,filename)\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_dataframe(filename, tocsv):\n",
    "    #name = input(\"Enter file with txt extension:\")\n",
    "\n",
    "    #tocsv = input(\"name to save:\")\n",
    "    \"\"\" 1.This function reads filename, that the user provided with an extension .txt generated from pubmed after get_data(search_for, filename) function.\n",
    "        2.Then parses the xml file and creates a lists of PMID, Article_title, ISOAbbreviation, Journal_title,\n",
    "         Abstract, Journal_Country,Published_year, Keyword_list,publication_type,Medlinecitation,pubmed_year,Affiliation\n",
    "        3.Take the above list and creates a dataframe and returns a dataframe\n",
    "        4.Finally the datafrane is saved into a csv file\"\"\"\n",
    "\n",
    "    files = glob.glob('../data/output/*.txt')\n",
    "    PMID = []\n",
    "    year=[]\n",
    "    ISO = []\n",
    "    Article_title = []\n",
    "    Journal_Country=[]\n",
    "    Journal_title=[]\n",
    "    abstract = []\n",
    "    keywords=[]\n",
    "    Medlinecitation = []\n",
    "    pubmed_year =[]\n",
    "    pubtype=[]\n",
    "    affiliation=[]\n",
    "    for file in tqdm(files):\n",
    "        with open(file, 'r') as reader:\n",
    "            contents = reader.read()\n",
    "            soup = BeautifulSoup(contents, 'xml')\n",
    "            root = soup.find_all('PubmedArticle')\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "            notuseful_list = ['Research Support', \"U.S. Gov't\",\"Non-U.S. Gov't\",\"Research Support, Non-U.S. Gov't\",\n",
    "           \"Research Support, N.I.H., Extramural\", \"Research Support, U.S. Gov't, Non-P.H.S.\",\n",
    "           \"Research Support, N.I.H., Extramural,Research Support, U.S. Gov't, Non-P.H.S.\" ,\n",
    "           \"Research Support, N.I.H., Intramural\", \"Research Support, U.S. Gov't, P.H.S.\" ] \n",
    "\n",
    "\n",
    "            for item in root:\n",
    "                pmid =  item.find('PMID')\n",
    "                pmid_text= pmid.text\n",
    "                PMID.append(pmid_text)\n",
    "                title = item.find('ArticleTitle')\n",
    "                title_text = title.text\n",
    "                Article_title.append(title_text)\n",
    "            for item in root:\n",
    "                iso_abbreviation = item.find('ISOAbbreviation')\n",
    "                if iso_abbreviation is not None:\n",
    "                    iso_abbreviation_text = iso_abbreviation.text\n",
    "                    ISO.append(iso_abbreviation_text)\n",
    "\n",
    "            for item in root:\n",
    "                if item is not None:\n",
    "                    journal = item.find('Journal')\n",
    "                    journal_name = journal.find_all('Title')\n",
    "                    for item in journal_name:\n",
    "                        journal_name_list = item.string\n",
    "                        Journal_title.append(journal_name_list)\n",
    "                else:\n",
    "                     Journal_title.append(None)  \n",
    "\n",
    "            all_Year_info =[]\n",
    "            for item in root:\n",
    "                all_Year_info =[]\n",
    "                year_pub =  item.find_all('PubDate')\n",
    "                year_pub_text = year_pub[0].text\n",
    "                all_Year_info.append(year_pub_text)\n",
    "                s = ''.join(all_Year_info)\n",
    "            for item in re.findall('(\\d{4})', s):\n",
    "                year.append(item.strip())\n",
    "\n",
    "            for item in root:\n",
    "                year_pub =  item.find(PubStatus=\"pubmed\")\n",
    "                if year_pub is not None:\n",
    "                    year1 =  year_pub.find_all('Year')\n",
    "                    for i in year1:\n",
    "                        pubmed_year.append(i.text)\n",
    "\n",
    "            \n",
    "            for item in root:\n",
    "                pub = item.find('PublicationTypeList')\n",
    "                if pub is not None:\n",
    "                    pub_lst=[]\n",
    "                    pubtype_list = pub.find_all('PublicationType')\n",
    "                    for item in pubtype_list:\n",
    "                        pubtype_text = item.text\n",
    "                        pub_lst.append(pubtype_text)\n",
    "                    pub_lst = [x for x in pub_lst if x.strip() not in notuseful_list]\n",
    "                    pubs_join= ','.join(pub_lst)\n",
    "                    pubtype.append(pubs_join)\n",
    "                else:\n",
    "                    pubtype.append(None)      \n",
    "\n",
    "            for item in root:\n",
    "                journal_country = item.find('MedlineJournalInfo')\n",
    "                if journal_country is not None:\n",
    "                    country_list = journal_country.find_all('Country')\n",
    "                    for item in country_list:\n",
    "                        country_list=item.text\n",
    "                        Journal_Country.append(country_list)\n",
    "                else:\n",
    "                    Journal_Country.append(None)\n",
    "\n",
    "            for item in root:\n",
    "                abstract_text = item.find('Abstract')\n",
    "                if abstract_text is not None:\n",
    "                    text = abstract_text.find_all('AbstractText')\n",
    "                    lst = []\n",
    "                    for item in text:\n",
    "                        lst.append(item.text)\n",
    "                    lst_join='\\n'.join(lst)\n",
    "                    abstract.append(lst_join)\n",
    "                else:\n",
    "                     abstract.append(None) \n",
    "\n",
    "            for item in root:\n",
    "                keyword_text=item.find('KeywordList')\n",
    "                if keyword_text is not None:\n",
    "                    key=[]\n",
    "                    keyword_text_list=keyword_text.find_all('Keyword')\n",
    "                    for item in keyword_text_list:\n",
    "                        keyword_text=item.text\n",
    "                        key.append(keyword_text)\n",
    "                    keys_join=','.join(key)\n",
    "                    keywords.append(keys_join)\n",
    "                else:\n",
    "                    keywords.append(None)\n",
    "\n",
    "            for item in soup.find_all('MedlineCitation'):\n",
    "                status = item.get('Status')\n",
    "                Medlinecitation.append(status)    \n",
    "\n",
    "            \n",
    "            for item in root:\n",
    "                abstract_text = item.find('AuthorList')\n",
    "                if abstract_text is not None:\n",
    "                    text = abstract_text.find_all('Affiliation')\n",
    "                    lst = []\n",
    "                    for item in text:\n",
    "                        lst.append(item.text)\n",
    "                    lst_join='\\n'.join(lst).replace(\"\\n\",\"\")\n",
    "                    affiliation.append(lst_join)\n",
    "                else:\n",
    "                    affiliation.append(None)\n",
    "\n",
    "            dict_columns = {'PMID': PMID,\n",
    "                            'Title': Article_title,\n",
    "                            'ISOAbbreviation': ISO,\n",
    "                            'journal_title':Journal_title,\n",
    "                            'Abstract':abstract,\n",
    "                            'Journalinfo_country': Journal_Country,\n",
    "                            'Published_year':all_Year_info,\n",
    "                            'Keyword_list':keywords,\n",
    "                            'publication_type':pubtype,\n",
    "                            'medline_citation':Medlinecitation,\n",
    "                            \"pubmed_year\":pubmed_year,\n",
    "                            \"Affiliation\":affiliation}\n",
    "\n",
    "            df =pd.DataFrame.from_dict(dict_columns, orient='index').transpose()\n",
    "            df.to_csv('../data/output/'+tocsv+'.csv',index=False)\n",
    "            print(\"Number of articles :\"+str(len(root)))\n",
    "            \n",
    "\n",
    "\n",
    "get_dataframe(filename, tocsv)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
